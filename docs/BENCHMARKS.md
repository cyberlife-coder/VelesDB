# ğŸ“Š VelesDB Performance Benchmarks

*Last updated: February 1, 2026 (v1.4.1 - SIMD Tiered Dispatch EPIC-052/077)*

---

## ğŸš€ SIMD Performance Results (Post-EPIC-052)

### Hardware Configuration
- **CPU**: Intel Core i9-14900K (24 cores, 32 threads, AVX2 native)
- **RAM**: 64GB DDR5
- **GPU**: NVIDIA RTX 4090 (for GPU benchmarks)
- **OS**: Windows 11 (Power Mode: "Performances Ã©levÃ©es")
- **Rust**: 1.85, `--release`, `target-cpu=native`
- **Tests**: 2411 passing, 82.30% coverage

### SIMD Kernel Benchmarks (LTO thin, codegen-units=1)

| Operation | 128D | 384D | **768D** | **1536D** | **3072D** |
|-----------|------|------|----------|-----------|-----------|
| **dot_product** | 4.05ns | 9.71ns | **18.68ns** | **32.91ns** | **70.73ns** |
| **euclidean** | 8.59ns | 11.56ns | **20.88ns** | 43.80ns | 81.69ns |
| **cosine** | 7.87ns | 19.67ns | **37.26ns** | 58.09ns | 110.13ns |
| **hamming** | 6.25ns | 9.78ns | **18.99ns** | 38.35ns | 82.01ns |
| **jaccard** | 5.00ns | 11.61ns | **22.81ns** | 47.72ns | 93.63ns |

### ğŸ“ˆ Throughput Analysis

| Dimension | Dot Product | Throughput |
|-----------|-------------|------------|
| 768D | 18.68ns | **41.1 Gelem/s** |
| 1536D | 32.91ns | **46.6 Gelem/s** |
| 3072D | 70.73ns | **43.4 Gelem/s** |

### ğŸ¯ Key Achievements

#### âœ… Major Performance Gains (EPIC-052/077)
- **Dot Product**: 18.5ns @ 768D â†’ **41.6 Gelem/s**
- **Cosine tiered dispatch**: 2-acc (64-1023D) + 4-acc (>1024D) pour Ã©viter register pressure
- **Jaccard**: 22.8ns @ 768D (avant 28.1ns)
- **Hamming**: 19.0ns @ 768D (avant 36.2ns)

---

## ğŸ”„ HNSW Insert Performance

| Operation | Vectors | Time | Throughput |
|-----------|---------|------|------------|
| **Sequential Insert** | 1,000 Ã— 768D | 614ms | **1,628 vec/s** |
| **Parallel Insert** | 1,000 Ã— 768D | 443ms | **2,259 vec/s** |

**Parallel insert** provides **38% speedup** over sequential.

---

## ğŸŒ Competitive Analysis (State of the Art 2025)

### SIMD Distance Kernels

| Library | Dot Product 1536D | Notes |
|---------|-------------------|-------|
| **VelesDB** | **32ns** | AVX2 4-acc, native Rust |
| SimSIMD | ~25-30ns | AVX-512, C library |
| NumPy | ~200-400ns | BLAS backend |
| SciPy | ~300-500ns | No SIMD optimization |

**VelesDB** is **competitive with SimSIMD** and **10-15x faster than NumPy/SciPy**.

### Vector Database Search Latency

| Database | Search Latency | Scale | Notes |
|----------|---------------|-------|-------|
| **VelesDB** | **< 1ms** | 10K | Local, in-memory HNSW |
| Milvus | < 10ms p50 | 1M+ | Distributed |
| Qdrant | 20-50ms | 1M+ | Cloud/distributed |
| pgvector | 45-100ms | 100K+ | PostgreSQL extension |
| Redis | ~5ms | 1M+ | In-memory |

**VelesDB excels for local/embedded use cases** with sub-millisecond latency.

### Insert Throughput

| Database | Insert Rate | Notes |
|----------|-------------|-------|
| **VelesDB** | **2,259 vec/s** | Single machine, parallel |
| Milvus | Highest indexing | Distributed, batch |
| Qdrant | ~1,000 vec/s | Single node |

---

## ğŸ¯ VelesDB Positioning

### âœ… Where VelesDB Excels
1. **Local-first / Edge**: Sub-ms latency, no network overhead
2. **Embedded**: 15MB binary, zero dependencies
3. **SIMD Performance**: Competitive with state-of-the-art
4. **Privacy**: Data never leaves device

### ğŸ“ˆ Optimization Opportunities
1. **Batch Insert**: Implement batch indexing for higher throughput
2. **AVX-512**: Enable on supported hardware (i9-14900K has AVX2 only)
3. **Quantization**: int8/int4 vectors for memory efficiency
4. **GPU Acceleration**: CUDA/WebGPU for large-scale search

---

## ğŸš€ v1.2.0 Headline

| Metric | Baseline | VelesDB | Winner |
|--------|----------|---------|--------|
| **SIMD Dot Product (1536D)** | 280ns (Naive) | **110ns** | **VelesDB 2.5x** âœ… |
| **HNSW Search (10K/768D)** | ~50ms (pgvector) | **57Âµs** | **VelesDB 877x** âœ… |
| **ColumnStore Filter (100K)** | 3.9ms (JSON) | **88Âµs** | **VelesDB 44x** âœ… |
| **VelesQL Parse** | N/A | **84ns** (cache) | **VelesDB** âœ… |
| **Recall@10** | 100% | **100%** | **VelesDB Perfect** âœ… |

### When to Choose VelesDB

- âœ… **Ultra-low latency** â€” Microsecond-level search on local datasets
- âœ… **Embedded/Desktop** â€” Native Rust integration with zero network overhead
- âœ… **On-Prem/Edge** â€” Single binary, no dependencies
- âœ… **WASM/Browser** â€” Client-side vector search capability

### When to Choose pgvector

- âœ… Existing PostgreSQL infrastructure
- âœ… Need 100% recall

---

## âš¡ SIMD Performance Summary (i9-14900K AVX2 4-acc)

| Operation | 384D | 768D | 1536D | vs v1.4.0 |
|-----------|------|------|-------|-----------|
| **Dot Product** | **9.7ns** | **18.7ns** | **32.9ns** | **Baseline** |
| **Euclidean** | 13.4ns | 20.9ns | 43.8ns | **Improved** |
| **Cosine** | 19.7ns | 37.3ns | 58.1ns | **-13%** âœ… |

### StratÃ©gie Adaptative (EPIC-PERF-003) - OptimisÃ©e Feb 2026

Le dispatch s'adapte automatiquement au CPU dÃ©tectÃ© avec des seuils optimisÃ©s basÃ©s sur la recherche state-of-the-art:

| CPU DÃ©tectÃ© | ImplÃ©mentation | Seuils | Gain typique |
|-------------|----------------|--------|--------------|
| **AVX-512** (Xeon, serveurs) | 512-bit 4-acc | >= 512 Ã©lÃ©ments | 15-25% |
| **AVX2** (Core 12th/13th/14th gen, Ryzen) | 256-bit 4-acc | >= 256 | 15-37% |
| **AVX2** | 256-bit 2-acc | 64-255 | Baseline |
| **AVX2 petits vecteurs** | 256-bit 1-acc | **16-63** | **Meilleur ratio overhead/perf** |
| **AVX2 tiny** | Scalar | **< 16** | Ã‰vite overhead SIMD |
| **ARM NEON** | 128-bit 1-acc | >= 4 | Baseline |

**Optimisations implÃ©mentÃ©es:**
- **Tail unrolling**: Remainder dÃ©roulÃ© (4â†’2â†’1 Ã©lÃ©ments) pour Ã©viter les boucles
- **Warmup AVX-512**: 3 itÃ©rations avant mesure pour stabiliser la frÃ©quence CPU
- **Dispatch optimisÃ©**: Scalar < 16 Ã©lÃ©ments (Ã©vite overhead SIMD setup)

### EPIC-073 SIMD Pipeline Optimizations

| Feature | Description | Performance |
|---------|-------------|-------------|
| **Multi-level Prefetch** | L1/L2/L3 cache hints | 10-30% cold cache improvement |
| **Jaccard 4-way ILP** | Instruction-level parallelism | **2.3x** faster than baseline |
| **Binary Jaccard POPCNT** | Hardware popcount | **10x** faster for u64 packed |
| **Batch Dot Product** | MÃ—N matrix computation | Amortized overhead |
| **Batch Top-K** | Multi-query similarity | Cache reuse optimization |

---

## ğŸ” HNSW Vector Search

| Operation | Latency | Throughput |
|-----------|---------|------------|
| **Search k=10** | 57Âµs | 9.2K qps |
| **Search k=50** | 90Âµs | - |
| **Search k=100** | 174Âµs | - |
| **Insert 1KÃ—768D** | 696ms | 1.4K elem/s |

---

## ğŸ” ColumnStore Filtering

| Scale | ColumnStore | JSON | Speedup |
|-------|-------------|------|---------|
| 10K rows | 8.6Âµs | 397Âµs | **46x** |
| 100K rows | 88Âµs | 3.9ms | **44x** |
| 500K rows | 136Âµs | 18.6ms | **137x** |

---

## ğŸ“ VelesQL Parser

| Mode | Latency | Throughput |
|------|---------|------------|
| Simple Parse | 1.4Âµs | 707K qps |
| Vector Query | 2.0Âµs | 490K qps |
| Complex Query | 7.9Âµs | 122K qps |
| **Cache Hit** | **84ns** | **12M qps** |
| EXPLAIN Plan | 61ns | 16M qps |

```rust
use velesdb_core::velesql::QueryCache;
let cache = QueryCache::new(1000);
let query = cache.parse("SELECT * FROM docs LIMIT 10")?;
```

---

## ğŸ“ˆ HNSW Recall Profiles (10K/128D)

| Profile | Recall@10 | Latency P50 | Change vs v1.0 |
|---------|-----------|-------------|----------------|
| Fast (ef=64) | 92.2% | **36Âµs** | ğŸ†• new |
| Balanced (ef=128) | 98.8% | **57Âµs** | ğŸš€ **-80%** |
| Accurate (ef=256) | 100.0% | **130Âµs** | ğŸš€ **-72%** |
| **Perfect (ef=2048)** | **100%** | **200Âµs** | ğŸš€ **-92%** |

> **Note**: Recall@10 â‰¥95% guaranteed for Balanced mode and above.
> 
> **v1.1.0 Performance Gains**: EPIC-CORE-003 optimizations (LRU Cache, Trigram Index, Lock-free structures) delivered **72-92% latency improvements** across all modes.

### âš ï¸ Benchmark Interpretation Note

**Criterion benchmarks** measure **batch execution time** (100 queries total). To get **per-query latency**, divide by 100:

| Mode | Criterion Output | Per-Query Latency | Calculation |
|------|-----------------|-------------------|-------------|
| Fast | 3.6ms | **36Âµs** | 3.6ms Ã· 100 |
| Balanced | 5.7ms | **57Âµs** | 5.7ms Ã· 100 |
| Accurate | 13ms | **130Âµs** | 13ms Ã· 100 |
| Perfect | 20ms | **200Âµs** | 20ms Ã· 100 |

When comparing with other vector databases or previous VelesDB versions, always use **per-query latency** for accurate comparison.

---

## ğŸš€ Parallel Performance

| Operation | Speedup (8 cores) |
|-----------|------------------|
| Batch Search | **19x** |
| Batch Insert | **18x** |

---

## ğŸ¯ Performance Targets by Scale

| Dataset Size | Search P99 | Recall@10 | Status |
|--------------|------------|-----------|--------|
| 10K vectors | **<1ms** | â‰¥98% | âœ… Achieved |
| 100K vectors | **<5ms** | â‰¥95% | âœ… Achieved (96.1%) |
| 1M vectors | **<50ms** | â‰¥95% | ğŸ¯ Target |

> Use `HnswParams::for_dataset_size()` for automatic parameter tuning.

---

## ğŸ†• v0.8.12 Native HNSW Implementation

VelesDB now includes a **custom Native HNSW implementation** based on 2024-2026 research papers (Flash Method, VSAG Framework).

### Native vs hnsw_rs Comparison

*Benchmarked January 8, 2026 â€” 5,000 vectors, 128D, Euclidean distance*

| Operation | Native HNSW | hnsw_rs | Improvement |
|-----------|-------------|---------|-------------|
| **Search (100 queries)** | 26.9 ms | 32.4 ms | **1.2x faster** âœ… |
| **Parallel Insert (5k)** | 1.47 s | 1.57 s | **1.07x faster** âœ… |
| **Recall** | ~99% | baseline | Parity âœ“ |

### Why Native HNSW?

- **No external dependency** â€” Full control over graph construction and search
- **SIMD-optimized distances** â€” Custom AVX2/SSE implementations
- **Lock-free reads** â€” Concurrent search without blocking
- **Future-ready** â€” Foundation for int8 quantized graph traversal

```bash
# Enable Native HNSW
cargo build --features native-hnsw

# Run comparison benchmark
cargo bench --bench hnsw_comparison_benchmark
```

ğŸ“– Full guide: [docs/reference/NATIVE_HNSW.md](reference/NATIVE_HNSW.md)

---

## ğŸ”¥ v0.8.5 Optimizations

- **Unified VelesQL execution** â€” `Collection::execute_query()` for all components
- **Batch search with filters** â€” Individual filters per query in batch operations
- **Buffer reuse** â€” Thread-local buffer for brute-force search (~40% allocation reduction)
- **Adaptive HNSW params** â€” `for_dataset_size()` and `million_scale()` APIs
- **32-wide SIMD unrolling** â€” 4x f32x8 accumulators for maximum ILP
- **Pre-normalized functions** â€” `cosine_similarity_normalized()` ~40% faster
- **SIMD-accelerated HNSW** â€” AVX2/SSE via `wide` crate
- **Parallel insertion** â€” Rayon-based graph construction
- **CPU prefetch hints** â€” L2 cache warming
- **GPU acceleration** â€” [Roadmap](GPU_ACCELERATION_ROADMAP.md) for batch operations

---

## ğŸ”— Graph (EdgeStore)

| Operation | Latency |
|-----------|---------|
| **get_neighbors (degree 10)** | 155ns |
| **get_neighbors (degree 50)** | 508ns |
| **add_edge** | 278ns |
| **BFS depth 3** | 3.6Âµs |
| **Parallel reads (8 threads)** | 346Âµs |

---

## ğŸ§ª Methodology

- **Hardware**: 8-core CPU, 32GB RAM
- **Environment**: Rust 1.85, `--release`, `target-cpu=native`
- **Framework**: Criterion.rs
