---
phase: 5
plan: 2
name: README Metrics, Numbers & Ecosystem Truth
wave: 1
depends_on: none
autonomous: true
parallel_safe: true
---

# Phase 5 Plan 02: README Metrics, Numbers & Ecosystem Truth

## Objective

Audit and correct all quantitative claims in README.md ‚Äî test counts, coverage percentages, performance numbers, version references, and ecosystem component links. Every number in the README must be verifiable and current.

## Context

**Requirements addressed:** VP-008 (README accuracy audit)
**Phase goal contribution:** Users and evaluators trust numbers. Stale or conflicting metrics damage credibility more than missing features.

**Current issues identified:**

| Issue | Location | Current | Actual |
|-------|----------|---------|--------|
| Test count badge | Line 28 | "3,100+" | 3,222 (STATE.md) |
| Quality Gates tests | Line 142 | "3,000 passing" | 3,222 |
| Header performance | Line 11 | "18.7ns SIMD" | Verify ‚Äî may be specific benchmark |
| Core ops table | Line 1048 | "46 ns" Dot Product | Different measurement than header? |
| Performance by Numbers | Line 120 | "18.7 ns" Dot Product | Conflicts with line 1048 |
| Codebase LoC | Line 131 | "~133,000" | Verify with tokei/cloc |
| Benchmarks count | Line 133 | "35 criterion suites" | Verify |
| Coverage | Line 29, 99, 145, 1063 | "82.30%" | Verify against latest |
| v1.4.0 test count | Line 1346 | "2,765 tests" | Correct for that release |
| Quality Gates coverage | Line 145 | "> 75% (82.30%)" | Verify |

## Tasks

### Task 1: Verify and Update Test Count & Coverage Numbers

**Files:** `README.md`

**Action:**
1. Run `cargo test --workspace 2>&1 | Select-String "test result"` to get current test count
2. Update ALL locations where test count appears:
   - Line 11: header tagline `3,100+ Tests`
   - Line 28: badge `üß™_Tests-3,100+`
   - Line 94: quality metrics `üß™ 3,100+`
   - Line 142: quality gates `cargo test --workspace (3,000 passing)`
   - Line 1063: system performance `3,000+ tests`
3. For coverage: check if `coverage_summary.txt` exists at repo root and use that number
4. Update coverage in all locations:
   - Line 11: header `82% Coverage`
   - Line 29: badge
   - Line 98-99: quality metrics
   - Line 145: quality gates
   - Line 1063: system performance
5. Use consistent rounding: if 3,222 tests ‚Üí use "3,200+" in badges (round down to nearest hundred for badges, exact in text)

**Verify:**
```powershell
cargo test --workspace 2>&1 | Select-String "test result"
Select-String -Path "README.md" -Pattern "3,1[0-9]+" # Should find 0 matches after fix
```

**Done when:**
- All test count references are consistent and current
- All coverage references are consistent and current
- No stale "3,100" or "3,000" references remain

---

### Task 2: Reconcile Performance Numbers

**Files:** `README.md`

**Action:**
1. Identify the source of performance discrepancy:
   - Header/badge: "18.7ns" Dot Product
   - "Performance by the Numbers" table (line 120): "18.7 ns" Dot Product 768D
   - "Core Vector Operations" table (line 1048): "46 ns" Dot Product 768D
   - These CANNOT both be correct for the same benchmark
2. Check benchmark results files: `bench_simd_results.txt`, `benchmark_results.txt`
3. Determine which number is from which hardware/configuration:
   - 18.7ns may be from i9-14900KF with AVX2 4-acc optimization (see SIMD memory)
   - 46ns may be from a different/older benchmark run
4. Resolution strategy:
   - Use the LATEST verified benchmark numbers consistently
   - If different hardware: add context (e.g., "18.7ns on i9-14900KF")
   - Remove duplicate/conflicting tables or reconcile with clear labels
5. Also verify HNSW search latency (57¬µs), ColumnStore filter (88¬µs), VelesQL cache (84ns)

**Verify:**
```powershell
# Check benchmark result files
Get-Content "bench_simd_results.txt" | Select-String "dot_product"
Get-Content "benchmark_results.txt" | Select-String "dot|hnsw" -CaseSensitive:$false
```

**Done when:**
- No conflicting performance numbers in README
- Each number has clear context (hardware, vector size, configuration)
- Performance tables are consistent with each other

---

### Task 3: Audit Ecosystem Table & Component Links

**Files:** `README.md`

**Action:**
1. Verify each ecosystem component exists and link is valid:

   | Component | Link | Exists? |
   |-----------|------|---------|
   | velesdb-core | `crates/velesdb-core` | ‚úÖ Verified |
   | velesdb-server | `crates/velesdb-server` | ‚úÖ Verified |
   | velesdb-cli | `crates/velesdb-cli` | ‚úÖ Verified |
   | velesdb-python | `crates/velesdb-python` | ‚úÖ Verified |
   | typescript-sdk | `sdks/typescript` | ‚úÖ Verified |
   | velesdb-wasm | `crates/velesdb-wasm` | ‚úÖ Verified |
   | velesdb-mobile | `crates/velesdb-mobile` | ‚úÖ Verified |
   | tauri-plugin | `crates/tauri-plugin-velesdb` | ‚úÖ Verified |
   | langchain | `integrations/langchain` | ‚úÖ Verified |
   | llamaindex | `integrations/llamaindex` | ‚úÖ Verified |
   | velesdb-migrate | `crates/velesdb-migrate` | ‚úÖ Verified |

2. Verify install commands are plausible:
   - `cargo add velesdb-core` ‚Äî is it published on crates.io?
   - `pip install velesdb` ‚Äî is it published on PyPI?
   - `npm i @wiscale/velesdb` ‚Äî is it published on npm?
   - If NOT published, add note: "(from source)" or "(coming soon to registry)"

3. Verify demo links:
   - `examples/wasm-browser-demo/` exists ‚úÖ
   - `demos/rag-pdf-demo/` ‚Äî verify exists
   - `demos/tauri-rag-app/` ‚Äî verify exists

4. Check for phantom references (components mentioned but don't exist)

5. Update "Crates" count (line 132) ‚Äî verify it's actually 8 production crates

**Verify:**
```powershell
# Verify all linked paths exist
@("crates/velesdb-core","crates/velesdb-server","crates/velesdb-cli","crates/velesdb-python","sdks/typescript","crates/velesdb-wasm","crates/velesdb-mobile","crates/tauri-plugin-velesdb","integrations/langchain","integrations/llamaindex","crates/velesdb-migrate","examples/wasm-browser-demo","demos/rag-pdf-demo","demos/tauri-rag-app") | ForEach-Object { if (Test-Path $_) { "‚úÖ $_" } else { "‚ùå $_" } }
```

**Done when:**
- All ecosystem table links point to existing directories
- Install commands are accurate or annotated with "(from source)"
- No phantom components in the table
- Crate count is accurate

---

## Verification

After all tasks complete:

```powershell
# Check no stale test counts
Select-String -Path "README.md" -Pattern "3,0[0-9]{2}|3,1[0-9]{2}" | ForEach-Object { $_.LineNumber }
# Should return 0 or only in historical sections (v1.4.0 release notes)

# Check performance consistency
Select-String -Path "README.md" -Pattern "18\.7|46 ns" | ForEach-Object { "$($_.LineNumber): $($_.Line.Trim())" }
```

## Success Criteria

- [ ] All test count references updated to current count (consistent across file)
- [ ] All coverage references verified and consistent
- [ ] Performance numbers reconciled ‚Äî no conflicting claims
- [ ] Ecosystem table links all verified as existing
- [ ] Install commands accurate or properly annotated
- [ ] No phantom components referenced

## Parallel Safety

**Exclusive write files:** `README.md` (header, badges, metrics, ecosystem, performance sections)
**Shared read files:** `bench_simd_results.txt`, `benchmark_results.txt`, `coverage_summary.txt`
**Conflicts with:** none in Wave 1 (Plan 05-01 writes different file)

## Output

**Files modified:**
- `README.md` ‚Äî Updated metrics, numbers, ecosystem table, performance tables
