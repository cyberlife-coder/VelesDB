---
phase: 4.1
plan: 04
name: SSE Streaming + Full Regression Suite
wave: 4
depends_on: [04.1-03]
autonomous: true
parallel_safe: false
---

# Phase 4.1 Plan 04: SSE Streaming + Full Regression Suite

## Objective

Add `stream_traverse_graph()` as a Python generator to both integrations, completing the last missing feature. Then run the full regression suite across all 3 packages (common, LangChain, LlamaIndex) to verify zero regressions and 100% feature parity with the server API.

## Context

**Requirements addressed:** Audit gap #8 (stream_traverse_graph SSE)  
**Phase goal contribution:** Last of 10 missing features. After this plan, Python integrations have 100% feature parity with the velesdb-core server API (26/26 routes covered).

## Tasks

### Task 1: Add stream_traverse_graph() to LangChain VelesDBVectorStore

**Files:** `integrations/langchain/src/langchain_velesdb/vectorstore.py`

**Action:**
Add `stream_traverse_graph()` method after `traverse_graph()`:

**`stream_traverse_graph(source, max_depth=2, strategy="bfs", limit=100)`** — Instance method, returns `Iterator[Document]`.

- Validate: `validate_node_id(source)`, `validate_k(limit)`.
- Strategy must be "bfs" or "dfs".
- **If the Python `velesdb` SDK exposes a streaming traversal method** (e.g., `collection.stream_traverse()`):
  - Use it and `yield` Documents as they arrive.
  - Each yielded result → `Document(page_content=text, metadata={graph_depth, target_id, retrieval_mode: "stream"})`.
- **If the SDK does NOT expose streaming** (likely scenario):
  - Fall back to `self.traverse_graph()` and yield each Document from the list.
  - Add a comment: `# TODO: Replace with native SSE streaming when velesdb SDK supports it`.
  - This ensures the API exists and works, even without true SSE.
- Use `typing.Iterator` for return type annotation.
- Add `from typing import Iterator` to imports if not present.

Docstring example:
```python
def stream_traverse_graph(
    self,
    source: int,
    max_depth: int = 2,
    strategy: str = "bfs",
    limit: int = 100,
) -> Iterator[Document]:
    """Stream graph traversal results as a generator.

    Yields Documents one at a time as they are discovered during traversal.
    Memory-efficient for large graphs.

    Args:
        source: Starting node ID.
        max_depth: Maximum traversal depth (default: 2).
        strategy: Traversal strategy - "bfs" or "dfs" (default: "bfs").
        limit: Maximum nodes to return (default: 100).

    Yields:
        Document objects with graph_depth and target_id in metadata.

    Example:
        >>> for doc in store.stream_traverse_graph(source=42, max_depth=3):
        ...     print(f"Depth {doc.metadata['graph_depth']}: {doc.page_content}")
    """
```

**Verify:**
```bash
cd integrations/langchain && python -c "from langchain_velesdb import VelesDBVectorStore; print('stream_traverse_graph' in dir(VelesDBVectorStore))"
```

**Done when:**
- Method exists and returns `Iterator[Document]`
- Works as a Python generator (`yield`)
- Falls back gracefully if SDK doesn't support native streaming
- Inputs validated

---

### Task 2: Add stream_traverse_graph() to LlamaIndex VelesDBVectorStore

**Files:** `integrations/llamaindex/src/llamaindex_velesdb/vectorstore.py`

**Action:**
Add the same method to LlamaIndex, but yielding `NodeWithScore` instead of `Document`:

**`stream_traverse_graph(source, max_depth=2, strategy="bfs", limit=100)`** — Returns `Iterator[NodeWithScore]`.

- Same validation and delegation as LangChain.
- Each yielded result → `NodeWithScore(node=TextNode(...), score=depth_score)`.
- Score: `1.0 - (depth / (max_depth + 1))`.
- Same fallback strategy if SDK doesn't support native streaming.

Import `Iterator` from `typing` if not present.

**Verify:**
```bash
cd integrations/llamaindex && python -c "from llamaindex_velesdb import VelesDBVectorStore; print('stream_traverse_graph' in dir(VelesDBVectorStore))"
```

**Done when:**
- Method exists and returns `Iterator[NodeWithScore]`
- Same generator pattern as LangChain
- Depth-based scoring consistent with `traverse_graph()`

---

### Task 3: Add streaming tests

**Files:** `integrations/langchain/tests/test_streaming.py`, `integrations/llamaindex/tests/test_streaming.py`

**Action:**
Create new test files:

**LangChain tests (≥6 tests):**
- `test_stream_traverse_is_generator` — verify return is a generator (has `__next__`)
- `test_stream_traverse_yields_documents` — collect all yielded items, verify `List[Document]`
- `test_stream_traverse_metadata` — each doc has `graph_depth` and `target_id` in metadata
- `test_stream_traverse_validates_source` — SecurityError on negative node_id
- `test_stream_traverse_invalid_strategy` — ValueError on "invalid"
- `test_stream_traverse_no_collection` — ValueError when not initialized

**LlamaIndex tests (≥6 tests):** Mirror with `NodeWithScore`:
- Verify generator yields `NodeWithScore` objects
- Verify depth-based scores decrease with depth

Mock: same traversal data as Plan 03 tests.

**Verify:**
```bash
cd integrations/langchain && python -m pytest tests/test_streaming.py -v
cd integrations/llamaindex && python -m pytest tests/test_streaming.py -v
```

**Done when:**
- All 12+ tests pass
- Generator behavior verified (lazy evaluation)
- No server dependency

---

### Task 4: Full regression suite

**Files:** None modified — verification only.

**Action:**
Run the complete test suite across all 3 packages to confirm zero regressions:

```bash
# Common package
cd integrations/common && python -m pytest tests/ -v --tb=short

# LangChain (all tests including Plan 01/02/03/04)
cd integrations/langchain && python -m pytest tests/ -v --tb=short

# LlamaIndex (all tests including Plan 01/02/03/04)
cd integrations/llamaindex && python -m pytest tests/ -v --tb=short
```

**Expected baselines (before Phase 4.1):**
- Common: 95 pass
- LangChain: 59 pass, 20 skip
- LlamaIndex: 46 pass, 15 skip

**Expected after Phase 4.1 (all 4 plans):**
- Common: 95 + 8 (validators) = 103+ pass
- LangChain: 59 + 12 + 8 + 14 + 6 = 99+ pass, 20 skip
- LlamaIndex: 46 + 12 + 8 + 14 + 6 = 86+ pass, 15 skip

**Verify:**
Count pass/fail/skip for each suite. Zero failures required.

**Done when:**
- All 3 suites pass with zero failures
- Test counts match or exceed expected baselines
- No regressions in existing functionality

---

## Verification

After all tasks complete:

```bash
cd integrations/common && python -m pytest tests/ -v --tb=short
cd integrations/langchain && python -m pytest tests/ -v --tb=short
cd integrations/llamaindex && python -m pytest tests/ -v --tb=short
```

## Success Criteria

- [ ] `stream_traverse_graph()` returns `Iterator[Document]` in LangChain
- [ ] `stream_traverse_graph()` returns `Iterator[NodeWithScore]` in LlamaIndex
- [ ] Both use Python generators (`yield`)
- [ ] Graceful fallback if SDK doesn't support native SSE
- [ ] 12+ new streaming tests pass (6 per integration)
- [ ] Full regression: 0 failures across all 3 packages
- [ ] Common: 103+ tests, LangChain: 99+ tests, LlamaIndex: 86+ tests
- [ ] All 10 audit gaps now closed (26/26 server routes covered)

## Parallel Safety

**Exclusive write files:**
- `integrations/langchain/src/langchain_velesdb/vectorstore.py` — +1 method
- `integrations/langchain/tests/test_streaming.py` (new)
- `integrations/llamaindex/src/llamaindex_velesdb/vectorstore.py` — +1 method
- `integrations/llamaindex/tests/test_streaming.py` (new)

**Shared read files:**
- `integrations/common/src/velesdb_common/security.py` (validators)

**Conflicts with:** Plans 04.1-01/02/03 write to same vectorstore files → **sequential after Plan 03**

## Output

**Files modified:**
- `integrations/langchain/src/langchain_velesdb/vectorstore.py` — +1 method (stream_traverse_graph)
- `integrations/llamaindex/src/llamaindex_velesdb/vectorstore.py` — +1 method (stream_traverse_graph)

**Files created:**
- `integrations/langchain/tests/test_streaming.py` — 6+ tests
- `integrations/llamaindex/tests/test_streaming.py` — 6+ tests
