---
phase: 3
plan: 3
name: Quantization & DualPrecision Fixes
wave: 1
depends_on: none
autonomous: true
---

# Phase 3 Plan 3: Quantization & DualPrecision Fixes

## Objective

Make `DualPrecisionHnsw::search()` actually use int8 quantized traversal when the quantizer is trained (instead of silently falling back to f32). Fix `cosine_similarity_quantized` to avoid full dequantization. Rename `QuantizedVector` to eliminate naming collision.

## Context

**Requirements addressed:** B-04, B-06, D-08
**Phase goal contribution:** DualPrecision delivers on its promise (4x bandwidth reduction), quantized cosine is allocation-free, and naming is unambiguous.

**Current state:**
- `dual_precision.rs:search()` → calls `search_dual_precision()` → uses `self.inner.search()` (f32) for traversal. The real int8 path is only in `search_with_config()`.
- `cosine_similarity_quantized` dequantizes entire vector to compute norm
- `hnsw::native::quantization::QuantizedVector` collides with `scalar::QuantizedVector`

## Tasks

### Task 1: DualPrecision default search uses int8 [B-04]

**Files:** `crates/velesdb-core/src/index/hnsw/native/dual_precision.rs`

**Action:**
Modify `search_dual_precision()` to use the real int8 traversal path when quantizer is trained. Currently it does:
```rust
// BEFORE: always uses f32 traversal
fn search_dual_precision(&self, query: &[f32], k: usize, ef_search: usize) -> Vec<(NodeId, f32)> {
    let rerank_k = (ef_search * 2).max(k * 4);
    let candidates = self.inner.search(query, rerank_k, ef_search); // ← f32 always
    // ... re-rank ...
}
```

Change to delegate to the int8 traversal path:
```rust
fn search_dual_precision(&self, query: &[f32], k: usize, ef_search: usize) -> Vec<(NodeId, f32)> {
    // Use default config with int8 enabled
    let config = DualPrecisionConfig::default();
    self.search_with_config(query, k, ef_search, &config)
}
```

This ensures `search()` automatically uses int8 when quantizer is trained.

**Verify:**
```powershell
cargo test --package velesdb-core -- dual_precision
```

**Done when:**
- `search()` uses int8 traversal when quantizer is trained
- `search()` falls back to f32 only when quantizer is NOT trained
- `DualPrecisionConfig::default()` has `use_int8_traversal: true`
- All dual precision tests pass

---

### Task 2: cosine_similarity_quantized — no full dequant [B-06]

**Files:** `crates/velesdb-core/src/index/hnsw/native/quantization.rs`

**Action:**
Find the `cosine_similarity_quantized` function (or equivalent). Currently it dequantizes the entire vector to compute the norm:
```rust
// BEFORE: allocates Vec<f32> of size=dimension for norm computation
let dequantized: Vec<f32> = quantized.iter().map(|&q| scale * f32::from(q) + offset).collect();
let norm = dequantized.iter().map(|x| x * x).sum::<f32>().sqrt();
```

Replace with in-place computation on quantized data:
```rust
// AFTER: zero allocation — compute norm directly on int8 data
// norm² = Σ(scale * q[i] + offset)² = scale² * Σq[i]² + 2*scale*offset*Σq[i] + n*offset²
let sum_q_sq: i64 = quantized.iter().map(|&q| i64::from(q) * i64::from(q)).sum();
let sum_q: i64 = quantized.iter().map(|&q| i64::from(q)).sum();
let n = quantized.len() as f32;
let norm_sq = scale * scale * sum_q_sq as f32
    + 2.0 * scale * offset * sum_q as f32
    + n * offset * offset;
let norm = norm_sq.max(0.0).sqrt(); // clamp for numerical stability
```

This avoids `dimension × 4` bytes allocation per call.

**Verify:**
```powershell
cargo test --package velesdb-core -- quantization
```

**Done when:**
- No `Vec<f32>` allocation in quantized cosine path
- Norm computed directly on int8 data
- Results within 1e-4 tolerance of dequantized computation
- All quantization tests pass

---

### Task 3: Rename QuantizedVector → QuantizedVectorInt8 [D-08]

**Files:**
- `crates/velesdb-core/src/index/hnsw/native/quantization.rs`
- Any file importing `quantization::QuantizedVector`

**Action:**
1. Rename `QuantizedVector` → `QuantizedVectorInt8` in the HNSW native quantization module
2. Update all references across the codebase
3. This disambiguates from `scalar::QuantizedVector` which uses different quantization

**Verify:**
```powershell
# Check no ambiguous QuantizedVector remains
cargo check --package velesdb-core --features persistence

cargo test --package velesdb-core -- quantization
```

**Done when:**
- `QuantizedVectorInt8` in HNSW native module
- `scalar::QuantizedVector` unchanged (different module, different purpose)
- No naming collision
- All tests pass

---

## Verification

After all tasks complete:

```powershell
cargo test --package velesdb-core -- dual_precision
cargo test --package velesdb-core -- quantization
cargo test --workspace --features persistence,gpu,update-check --exclude velesdb-python
cargo fmt --all --check
cargo clippy --workspace --features persistence,gpu,update-check --exclude velesdb-python -- -D warnings
```

## Success Criteria

- [ ] `DualPrecisionHnsw::search()` uses int8 traversal when quantizer trained
- [ ] `cosine_similarity_quantized` allocates 0 extra bytes for norm computation
- [ ] Quantized norm within 1e-4 of dequantized computation
- [ ] `QuantizedVector` renamed to `QuantizedVectorInt8` in HNSW module
- [ ] No naming collision between quantization types
- [ ] 3,117+ existing tests still pass

## Output

**Files modified:**
- `crates/velesdb-core/src/index/hnsw/native/dual_precision.rs` — search delegates to int8 path
- `crates/velesdb-core/src/index/hnsw/native/quantization.rs` — zero-alloc norm, rename
- `crates/velesdb-core/src/index/hnsw/native/dual_precision_tests.rs` — verify int8 default
